{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88aa02f6-6d3b-4a27-97a2-1a582b135f80",
   "metadata": {},
   "source": [
    "Q1. What is anomaly detection and what is its purpose?\n",
    "--\n",
    "---\n",
    "Anomaly detection is a process in machine learning that identifies data points, events, and observations that deviate from a data set’s normal behavior. It's generally understood to be the identification of rare items, events, or observations which deviate significantly from the majority of the data and do not conform to a well-defined notion of normal behavior.\n",
    "\n",
    "The purpose of anomaly detection is manifold:\n",
    "- It's critical for industrial applications, especially for predictive and prescriptive maintenance.\n",
    "- It helps in defining system baselines, identifying deviations from that baseline, and investigating inconsistent data.\n",
    "- It's used to minimize risk factors, improve communication, and enable real-time anomaly monitoring.\n",
    "- It's important within financial data because it can indicate potential risks, control failures, or business opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bd90de-e081-42d2-9b48-ce9cc21f28e6",
   "metadata": {},
   "source": [
    "Q2. What are the key challenges in anomaly detection?\n",
    "--\n",
    "---\n",
    "Anomaly detection is a complex task and faces several challenges:\n",
    "\n",
    "1. Data Quality: The quality of the underlying dataset is the biggest driver in creating an accurate model. Problems can include null data or incomplete datasets, inconsistent data formats, duplicate data, different scales of measurement, and human error.\n",
    "\n",
    "2. Training Sample Sizes: If the training set is too small, the algorithm doesn’t have enough exposure to past examples to build an accurate representation of the expected value at a given time¹. Anomalies will skew the baseline, which will affect the overall accuracy of the model.\n",
    "\n",
    "3. Feature Extraction: Appropriate feature extraction is a challenge in anomaly detection.\n",
    "\n",
    "4. Defining Normal Behaviors: Defining what constitutes normal behavior can be difficult.\n",
    "\n",
    "5. Handling Imbalanced Distribution: There is often an imbalanced distribution of normal and abnormal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7932961-d426-47be-ad79-4dafce07f9d1",
   "metadata": {},
   "source": [
    "Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "---\n",
    "----\n",
    "The main difference between supervised and unsupervised anomaly detection lies in the approach and the type of data they deal with¹²³⁴:\n",
    "\n",
    "Supervised Anomaly Detection: This type of anomaly detection depends on predefined algorithms that have been used to train artificial intelligence systems. It requires labeled data where for each row you know if it is an outlier/anomaly or not. Any modeling technique for binary responses will work here, such as logistic regression or gradient boosting. The use of a supervised approach for anomaly detection is most common for rule-based recognition of outliers, especially for highly predictable and repetitive multivariate dataset patterns. However, its use is not recommendable where datasets are of high volume, complexity, or are defined by several multiple criteria. It can also be more demanding in terms of resources and maintenance, as engineers are required to continuously assess and modify algorithms.\n",
    "\n",
    "Unsupervised Anomaly Detection: This type of anomaly detection involves analyzing large amounts of anomalous and/or unlabeled data to identify areas of dis-uniformity, without using any predefined algorithm. The approach used in unsupervised anomaly detection is based on pattern-matching data-trend visualization; where data points are evaluated in clusters to identify general patterns of shapes, so that all outliers are identified as anomalies and used to diagnose a possible drawback. Unsupervised approach is better for anomaly detection than a supervised approach, because such tasks often involve multiple variables and relatively-high complexity, so that a definite, rule-based or supervised approach may not be effective in many cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fcaa3e-4597-459d-9d4e-c6a8927c5a5c",
   "metadata": {},
   "source": [
    "Q4. What are the main categories of anomaly detection algorithms?\n",
    "--\n",
    "----\n",
    "The main categories of anomaly detection algorithms are:\n",
    "\n",
    "   - Isolation Forest: This is an unsupervised anomaly detection algorithm that uses a random forest algorithm, or decision trees, under the hood to detect outliers in the dataset.\n",
    "   - Local Outlier Factor: This technique takes the density of data into account.\n",
    "   - Robust Covariance: This technique is used for anomaly detection.\n",
    "   - One-Class Support Vector Machine (SVM): This is a type of SVM used for anomaly detection.\n",
    "   - One-Class SVM with Stochastic Gradient Descent (SGD): This is a variation of the One-Class SVM that uses SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78e4cf-52fa-412e-af76-bcc5dc391e00",
   "metadata": {},
   "source": [
    "Q5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "--\n",
    "---\n",
    "The main assumptions made by distance-based anomaly detection methods are:\n",
    "\n",
    "1. Normal instances are related and appear close to each other: This assumption is based on the idea that normal data points in a dataset will have similar characteristics and therefore will be located near each other in the data space.\n",
    "\n",
    "2. Anomalies are different and relatively far from other instances: Anomalies, or outliers, are assumed to be different from the normal data points and therefore will be located far from the other data points.\n",
    "\n",
    "3. Anomaly score is calculated as a sum of the distance between a data point and its k-nearest neighbours: In distance-based methods, the anomaly score of a data point is calculated as a sum of the distance between a data point and its k-nearest neighbours.\n",
    "\n",
    "4. Normal data points are close to their neighbors, while the anomalous data points are far from the normal data: This assumption is based on the idea that normal data points will be close to their neighbors, while anomalous data points will be far from the normal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1e496a-207f-446c-b72b-a352886cda60",
   "metadata": {},
   "source": [
    "Q6. How does the LOF algorithm compute anomaly scores?\n",
    "--\n",
    "---\n",
    "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method. It computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors.\n",
    "\n",
    "Here's how it works:\n",
    "- The LOF algorithm measures the local density deviation of a given data point with respect to the data points near it.\n",
    "- It calculates the anomaly score representing how much a data point is considered an outlier in the dataset.\n",
    "- The anomaly score values greater than 1.0 usually indicate the anomaly.\n",
    "- The lower the density of the points, the more likely these points might be considered as outliers or anomalies.\n",
    "\n",
    "In essence, the LOF algorithm identifies those data points that have a substantially lower density than their neighbors as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb5e23-a8b2-4ab4-ac12-6e76aab0284b",
   "metadata": {},
   "source": [
    "Q7. What are the key parameters of the Isolation Forest algorithm?\n",
    "--\n",
    "----\n",
    "The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "1. n_estimators: The number of base estimators in the ensemble.\n",
    "2. max_samples: The number of samples to draw from X to train each base estimator.\n",
    "3. contamination: The amount of contamination of the data set, i.e., the proportion of outliers in the data set². This parameter is used when fitting to define the threshold on the scores of the samples.\n",
    "4. max_features: The number of features to draw from X to train each base estimator.\n",
    "5. bootstrap: If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed.\n",
    "6. n_jobs: The number of jobs to run in parallel for both fit and predict.\n",
    "7. random_state: Controls the pseudo-randomness of the selection of the feature and split values for each branching step and each tree in the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a2ae4-6cea-474d-8799-fcc4da195265",
   "metadata": {},
   "source": [
    "Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "--\n",
    "----\n",
    "The anomaly score for a data point in a K-nearest neighbors (KNN) algorithm is typically determined by the number of neighbors that belong to a different class. In your case, if a data point has only 2 neighbors of the same class within a radius of 0.5 and K is set to 10, then the anomaly score can be calculated as follows:\n",
    "\n",
    "1. If the majority of the neighbors (K/2 + 1) have the same class as the data point, the anomaly score is low.\n",
    "2. If the majority of the neighbors have a different class than the data point, the anomaly score is high.\n",
    "\n",
    "In this scenario, K is set to 10, so the majority would be at least 6 neighbors. If 6 or more out of the 10 nearest neighbors have the same class as the data point, the anomaly score would be low. If fewer than 6 neighbors have the same class, the anomaly score would be high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac3f9b7-d012-44e9-98f4-2c7ed042ef71",
   "metadata": {},
   "source": [
    "Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "--\n",
    "---\n",
    "The anomaly score for the data point, given the information you provided, is **0.1767766952966369**.\n",
    "\n",
    "Here's how we can calculate it:\n",
    "\n",
    "1. **Isolation Forest scores:** Isolation Forest algorithms typically assign anomaly scores between 0 and 1, where higher values indicate greater anomaly.\n",
    "2. **Path length ratio:** We need to compare the data point's path length (5.0) to the average path length of the trees. Let's say the average path length for all trees is 2.0.\n",
    "3. **Score formula:** The Isolation Forest anomaly score is calculated using the following formula:\n",
    "\n",
    "```\n",
    "anomaly_score = 2 ^ (-data_point_path_length / average_path_length)\n",
    "```\n",
    "\n",
    "4. **Plugging in values:** In your case, `data_point_path_length = 5.0` and `average_path_length = 2.0`. Substituting these values into the formula, we get:\n",
    "\n",
    "```\n",
    "anomaly_score = 2 ^ (-5.0 / 2.0) = 2 ^ (-2.5) = 0.1767766952966369\n",
    "```\n",
    "\n",
    "Therefore, the anomaly score for the data point is 0.1767766952966369, indicating a moderate anomaly compared to the data it's surrounded by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece27fb-0006-43a6-9def-cb032f13fcb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
